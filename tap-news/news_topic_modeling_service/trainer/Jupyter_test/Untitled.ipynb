{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import news_cnn_model\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from tensorflow.contrib.learn.python import SKCompat\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "\n",
    "stream = open(\"config.yml\", \"r\")\n",
    "load = yaml.load(stream)\n",
    "config = load['default']['news_topic_modeling_service']\n",
    "\n",
    "REMOVE_PREVIOUS_MODEL = config['trainer']['REMOVE_PREVIOUS_MODEL']\n",
    "\n",
    "MODEL_OUTPUT_DIR = config['key_config']['MODEL_DIR']\n",
    "DATA_SET_FILE = config['key_config']['Labeled_news_cvs_address']\n",
    "#RANDOM_DATA_SET_FILE = config['key_config']['Labeled_news_random_address']\n",
    "VARS_FILE = config['key_config']['VARS_FILE_ADDRESS']\n",
    "VOCAB_PROCESSOR_SAVE_FILE = config['key_config']['VOCAB_PROCESSOR_SAVE_FILE_ADDRESS']\n",
    "MAX_DOCUMENT_LENGTH = 300\n",
    "N_CLASSES = config['key_config']['CLASSES_NUMS']\n",
    "NUM_OF_TEST_DATA = config['key_config']['NUM_OF_TEST_DATA']\n",
    "# Training parms\n",
    "STEPS = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_clean( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and\n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters number\n",
    "    letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    #\n",
    "    # 5. Remove stop words 35.33\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Stem words(bad result) 24\n",
    "    stemming_words = [ps.stem(w) for w in meaningful_words]\n",
    "    #\n",
    "    # 7.lemmatize_words  30\n",
    "    lemmatize_words = [wnl.lemmatize(w) for w in stemming_words]\n",
    "    # 7. Join the words back into one string separated by space,\n",
    "    # and return the result.\n",
    "    return  ( \" \".join( stemming_words )).encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_col_process(data):\n",
    "    # Create an empty list and append the clean reviews one by one\n",
    "    clean_data = []\n",
    "    for i in data:\n",
    "        #print \"len of i %s\" % type(i)\n",
    "        clean_review = data_clean( str(i) )\n",
    "        clean_data.append( clean_review )\n",
    "        #print \"########### %s\" % type(clean_review)\n",
    "    #newDF = pd.DataFrame() #creates a new dataframe that's empty\n",
    "    #newDF = newDF.append(clean_data, ignore_index = True) # ignoring index is optional\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 9082\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f653b8d9110>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': ''}\n",
      "WARNING:tensorflow:From <ipython-input-23-251c527b487f>:52: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-23-251c527b487f>:52: calling fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From news_cnn_model.py:48: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into model/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.81894, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.917821\n",
      "INFO:tensorflow:loss = 0.116695, step = 101 (108.955 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.170158.\n",
      "WARNING:tensorflow:From <ipython-input-23-251c527b487f>:56: calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From news_cnn_model.py:48: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead. Note that the order of the logits and labels arguments has been changed.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:398: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Restoring parameters from model/model.ckpt-200\n",
      "Accuracy: 0.206667\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "    if REMOVE_PREVIOUS_MODEL:\n",
    "        # Remove old model\n",
    "        shutil.rmtree(MODEL_OUTPUT_DIR)\n",
    "        os.mkdir(MODEL_OUTPUT_DIR)\n",
    "\n",
    "    # Prepare training and testing data, encode UTF-8, uoting=3 tells Python to ignore doubled quotes\n",
    "    df = pd.read_csv(DATA_SET_FILE, header=None, encoding='utf8')\n",
    "\n",
    "    # Random shuffle\n",
    "    df.sample(frac=1)\n",
    "\n",
    "    #drop NaN value row\n",
    "    df = df.dropna(axis=0, how='any')\n",
    "    df.apply(lambda x: pd.api.types.infer_dtype(x.values))\n",
    "\n",
    "    #df = nltk.word_tokenize(str(df))\n",
    "\n",
    "    test_df = df[0:NUM_OF_TEST_DATA]\n",
    "    train_df = df.drop(test_df.index)\n",
    "\n",
    "    # x - 1 for news title 2 for news text, y - class\n",
    "    x_train = data_col_process(train_df[1]+train_df[2])\n",
    "    y_train = map(int, data_col_process(train_df[0]))\n",
    "    x_test = data_col_process(test_df[1]+test_df[2])\n",
    "    y_test = map(int, data_col_process(test_df[0]))\n",
    "\n",
    "    # Process vocabulary\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "    x_train = np.array(list(vocab_processor.fit_transform(x_train)))\n",
    "    x_test = np.array(list(vocab_processor.transform(x_test)))\n",
    "\n",
    "    #print x_train\n",
    "    #print x_test\n",
    "\n",
    "    n_words = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % n_words)\n",
    "    #print('Total words: %d' % n_words)\n",
    "\n",
    "    # Saving n_words and vocab_processor:\n",
    "    with open(VARS_FILE, 'w') as f:\n",
    "        pickle.dump(n_words, f)\n",
    "\n",
    "    vocab_processor.save(VOCAB_PROCESSOR_SAVE_FILE)\n",
    "\n",
    "    # Build model\n",
    "    classifier = learn.Estimator(\n",
    "        model_fn=news_cnn_model.generate_cnn_model(N_CLASSES, n_words),\n",
    "        model_dir=MODEL_OUTPUT_DIR)\n",
    "\n",
    "    # Train and predict\n",
    "    classifier.fit(x_train, y_train, steps=STEPS)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_predicted = [\n",
    "        p['class'] for p in classifier.predict(x_test, as_iterable=True)\n",
    "    ]\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, y_predicted)\n",
    "    print('Accuracy: {0:f}'.format(score))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main=main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
